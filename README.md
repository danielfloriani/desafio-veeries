# Desafio T√©cnico Veeries: Pipeline de Dados de Lineup Portu√°rio

## üìù Descri√ß√£o do Projeto
Este projeto implementa um pipeline de dados ETL (Extra√ß√£o, Transforma√ß√£o e Carga) em Python para coletar, processar e agregar diariamente os dados de lineup de navios dos portos de Santos e Paranagu√°. O objetivo final √© gerar uma base de dados anal√≠tica com os volumes di√°rios movimentados por porto, produto e sentido (importa√ß√£o/exporta√ß√£o).

## üèõÔ∏è Arquitetura
A solu√ß√£o foi desenvolvida utilizando a arquitetura Medallion, que separa os dados em tr√™s camadas l√≥gicas:
* **ü•â Camada Bronze:** Cont√©m os dados brutos, extra√≠dos diretamente das fontes, sem nenhuma altera√ß√£o (arquivos HTML). Isso garante a rastreabilidade e a capacidade de reprocessamento.
* **ü•à Camada Silver:** Armazena os dados ap√≥s um processo de limpeza, padroniza√ß√£o de esquema, convers√£o de tipos e unifica√ß√£o das diferentes fontes em um formato √∫nico e consistente (Parquet).
* **ü•á Camada Gold:** Apresenta os dados prontos para o consumo, agregados por dimens√µes de neg√≥cio para responder √† pergunta central do desafio: volumes di√°rios.

## üöÄ Como Executar
1.  **Clone o reposit√≥rio:**
    ```bash
    git clone [https://github.com/danielfloriani/lineup_ships]
    cd [lineup_ships]
    ```
2.  **Crie um ambiente virtual (recomendado):**
    ```bash
    python -m venv venv
    source venv/bin/activate  # No Windows: venv\Scripts\activate
    ```
3.  **Instale as depend√™ncias:**
    ```bash
    pip install -r requirements.txt
    ```
4.  **Execute o pipeline completo:**
    ```bash
    python pipeline.py
    ```
    Ao final da execu√ß√£o, os resultados estar√£o na pasta `data/`, com o arquivo final em `data/gold/volumes_diarios.parquet`.

### üõ†Ô∏è Estrutura do Projeto

O reposit√≥rio est√° organizado de forma modular para separar as diferentes responsabilidades do pipeline, seguindo as melhores pr√°ticas de engenharia de software.

* `data/`: Diret√≥rio que armazena os dados gerados pelo pipeline (ignorado pelo `.gitignore`).
    * `bronze/`: **Camada Bronze** - Armazena os dados brutos e inalterados, extra√≠dos diretamente das fontes (arquivos HTML).
    * `silver/`: **Camada Silver** - Cont√©m os dados ap√≥s a primeira etapa de limpeza e consolida√ß√£o (arquivo Parquet).
    * `gold/`: **Camada Gold** - Guarda os dados finais, agregados e prontos para an√°lise de neg√≥cio (arquivo Parquet).

* `src/`: Diret√≥rio principal que cont√©m todo o c√≥digo-fonte da aplica√ß√£o.
    * `common/`: M√≥dulos com fun√ß√µes utilit√°rias reutilizadas em v√°rias partes do projeto (ex: `fetch_page`).
    * `config/`: Arquivos de configura√ß√£o, sem l√≥gica de neg√≥cio (ex: `settings.py` para caminhos e `mappings.py` para dicion√°rios).
    * `extract/`: M√≥dulos respons√°veis pela extra√ß√£o (scraping) dos dados de cada fonte. Cada arquivo corresponde a um porto.
    * `transform/`: M√≥dulos respons√°veis pela transforma√ß√£o dos dados (l√≥gica principal das camadas Silver e Gold).
    * `validation/`: Scripts e esquemas para valida√ß√£o da qualidade e integridade dos dados (ex: esquemas `Pandera`).

* `.gitignore`: Arquivo que define quais arquivos e pastas devem ser ignorados pelo Git.
* `pipeline.py`: Script principal (*entrypoint*) que orquestra a execu√ß√£o de todo o pipeline em sequ√™ncia.
* `requirements.txt`: Lista das depend√™ncias Python do projeto, permitindo a f√°cil recria√ß√£o do ambiente.
* `README.md`: Documenta√ß√£o completa do projeto (este arquivo).

## ü§î Hip√≥teses e Decis√µes de Projeto
* **Resili√™ncia a Falhas de SSL:** Foi implementado um fallback para tentativas de conex√£o sem verifica√ß√£o de certificado, pois foi detectado um problema de SSL em uma das fontes.
* **Manuten√ß√£o para Schema Drift:** O pipeline foi adaptado para lidar com uma mudan√ßa no layout do site do Porto de Santos (remo√ß√£o de uma coluna). A l√≥gica de extra√ß√£o agora √© mais robusta para lidar com colunas vazias.
* **Valida√ß√£o de Dados:** Foi utilizada a biblioteca `Pandera` para criar um script de valida√ß√£o (`validate_silver.py`) que garante a integridade estrutural dos dados consolidados antes do processamento final, atuando como um port√£o de qualidade.
* **Consolida√ß√£o de Colunas:** Foi desenvolvida uma fun√ß√£o auxiliar para consolidar colunas que, ap√≥s a renomea√ß√£o, ficavam duplicadas (ex: `sentido`, `produto`), garantindo um esquema limpo para a camada final.

## üîÆ Pr√≥ximos Passos e Melhorias
* **Orquestra√ß√£o:** Integrar o pipeline com um orquestrador de workflows como Airflow ou Prefect para agendamento, monitoramento e retentativas autom√°ticas.
* **Containeriza√ß√£o:** Empacotar a aplica√ß√£o com Docker para garantir a portabilidade e facilitar o deploy em diferentes ambientes.
* **Testes:** Expandir a su√≠te de testes unit√°rios para as fun√ß√µes de transforma√ß√£o e valida√ß√£o, garantindo a qualidade do c√≥digo.
* **Logging:** Substituir os comandos `print` por um sistema de logging mais robusto (ex: biblioteca `logging` do Python) para melhor controle e monitoramento dos eventos do pipeline.